{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d37e2260",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-24T17:45:06.839498Z",
     "iopub.status.busy": "2024-08-24T17:45:06.839055Z",
     "iopub.status.idle": "2024-08-24T17:45:13.909052Z",
     "shell.execute_reply": "2024-08-24T17:45:13.907474Z"
    },
    "papermill": {
     "duration": 7.078772,
     "end_time": "2024-08-24T17:45:13.912221",
     "exception": false,
     "start_time": "2024-08-24T17:45:06.833449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math,copy,re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# generate tensor of word embeddings for all possible words used (the vocabulary)\n",
    "# can be used to turn each input word to a vector, only done at first encoder\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "        # embed will be tensor of size vocabulary * dim of embedding\n",
    "        # each word can reference this array to get the respective embedding\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # generates vector of size batch_size * sentence_len * embed_dim\n",
    "        return self.embed(x)\n",
    "    \n",
    "    \n",
    "# generate and add positional encoding vectors to embedding vectors    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,max_seq_len,embed_model_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embed_dim = embed_model_dim\n",
    "        pe = torch.zeros(max_seq_len,embed_model_dim)\n",
    "        \n",
    "        for i in range(0,max_seq_len):\n",
    "            for j in range(0,embed_model_dim):\n",
    "                if (j % 2 != 0):\n",
    "                    pe[i, j] = math.sin(i / (10000 ** ((2 * j)/embed_model_dim)))\n",
    "                else:\n",
    "                    pe[i, j] = math.cos(i / (10000 ** ((2 * (j))/embed_model_dim)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # register as buffer so it is tracked correctly\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # scale\n",
    "        x = x * math.sqrt(self.embed_dim)\n",
    "        # add positional encoding using autograd \n",
    "        x = x + torch.autograd.Variable(self.pe[:,:x.size(1)], requires_grad=False)\n",
    "        return x\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d701724",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-24T17:45:13.923584Z",
     "iopub.status.busy": "2024-08-24T17:45:13.922707Z",
     "iopub.status.idle": "2024-08-24T17:45:13.942469Z",
     "shell.execute_reply": "2024-08-24T17:45:13.940816Z"
    },
    "papermill": {
     "duration": 0.029278,
     "end_time": "2024-08-24T17:45:13.945791",
     "exception": false,
     "start_time": "2024-08-24T17:45:13.916513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    # initialize sizes of num heads, sentence size, etc\n",
    "    # create linear mappings for different tensors that will be computed\n",
    "    def __init__(self, embed_dim=512, heads=8):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        # need to both be integers\n",
    "        self.embed_dim = embed_dim\n",
    "        self.heads = heads\n",
    "        \n",
    "        self.head_dim = embed_dim // heads\n",
    "\n",
    "        \n",
    "        # create tensors for the values/keys/queries to be used\n",
    "        self.values = nn.Linear(embed_dim, embed_dim)\n",
    "        self.keys = nn.Linear(embed_dim, embed_dim)\n",
    "        self.queries = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # fully connected layer\n",
    "        self.fully_connected_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    # get value, key, queries tensors\n",
    "    def generate_tensors(self, values, keys, query, N):\n",
    "        # fill tensors with values, keys,queries\n",
    "        values = self.values(values)  \n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(query)\n",
    "\n",
    "        # split embedding into num self.heads different pieces for val, key, queries\n",
    "        values = values.reshape(N, values.shape[1], self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, keys.shape[1], self.heads, self.head_dim)\n",
    "        queries = queries.reshape(N, query.shape[1], self.heads, self.head_dim)\n",
    "        \n",
    "        return values, keys, queries\n",
    "    \n",
    "    # run calculations on keys, queries, values\n",
    "    # mult queries by keys for diff words\n",
    "    def mat_mult(self, values, keys, query, mask, queries, N):\n",
    "        \n",
    "        q_k = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        # apply mask so that weights become 0\n",
    "        if mask is not None:\n",
    "            q_k = q_k.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # normalize out values so that they sum to 1 anddivide by scaling factor\n",
    "        # directly from youtube video\n",
    "        attention = torch.softmax(q_k / (self.embed_dim ** (1 / 2)), dim=3)\n",
    "\n",
    "        # mat multiply calculated attention by the values\n",
    "        attention = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values])\n",
    "        \n",
    "        # reshape to get rid of extra dimensions\n",
    "        attention = attention.reshape(\n",
    "            N, query.shape[1], self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        # run attention through fully connected layer \n",
    "        attention = self.fully_connected_layer(attention)\n",
    "\n",
    "        return attention\n",
    "        \n",
    "    # combine it all together\n",
    "    def forward(self, values, keys, query, mask=None):\n",
    "        values, keys, queries = self.generate_tensors(values, keys, query, query.shape[0])\n",
    "        return self.mat_mult(values, keys, query, mask, queries, query.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4d69ec8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-24T17:45:13.955521Z",
     "iopub.status.busy": "2024-08-24T17:45:13.954941Z",
     "iopub.status.idle": "2024-08-24T17:45:13.973795Z",
     "shell.execute_reply": "2024-08-24T17:45:13.972271Z"
    },
    "papermill": {
     "duration": 0.028159,
     "end_time": "2024-08-24T17:45:13.977365",
     "exception": false,
     "start_time": "2024-08-24T17:45:13.949206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# connects transformers' noramlization and attention calculations\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=4, heads=8, dropout=0.2):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.attention = SelfAttention(embed_dim, heads)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim) \n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # feed forward network that applies self attention to get a normalized tensor\n",
    "        self.feed_forward = nn.Sequential(\n",
    "                          nn.Linear(embed_dim, expansion_factor*embed_dim),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(expansion_factor*embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # connect layers of transformer block together\n",
    "    def forward(self,key,query,value, mask=None):\n",
    "        attention_out = self.attention(key, query, value, mask)  \n",
    "        x = attention_out + query \n",
    "        x = self.dropout(self.norm1(x)) \n",
    "\n",
    "        fwd = self.feed_forward(x) \n",
    "        fwd = fwd + x \n",
    "        return self.dropout(self.norm2(x)) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# connect transformer blocks to embedding/positional encoding to create the encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, seq_len, vocab_size, embed_dim, num_layers=2, expansion_factor=4, heads=8):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoder = PositionalEncoding(seq_len, embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, expansion_factor, heads) for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    # loop through all \n",
    "    def forward(self, x, mask=None):\n",
    "        # preliminary embedding \n",
    "        out = self.positional_encoder(self.embedding_layer(x))\n",
    "        # iterate through all transformer blocks\n",
    "        if mask is not None:\n",
    "            for layer in self.layers:\n",
    "                out = layer(out,out,out, mask)\n",
    "        else:\n",
    "            for layer in self.layers:\n",
    "                out = layer(out,out,out)\n",
    "\n",
    "        return out  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "570ee1f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-24T17:45:13.987957Z",
     "iopub.status.busy": "2024-08-24T17:45:13.987420Z",
     "iopub.status.idle": "2024-08-24T17:45:14.013696Z",
     "shell.execute_reply": "2024-08-24T17:45:14.012317Z"
    },
    "papermill": {
     "duration": 0.036801,
     "end_time": "2024-08-24T17:45:14.017977",
     "exception": false,
     "start_time": "2024-08-24T17:45:13.981176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# decoder components contanerized\n",
    "# builds off of the transformer block for simplicity\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=4, heads=8, dropout=0.2):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_dim, heads=8)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.transformer_block = TransformerBlock(embed_dim, expansion_factor, heads)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # run components\n",
    "    def forward(self, key, query, x, mask=None):\n",
    "        \n",
    "        attention = self.attention(x,x,x,mask=mask)\n",
    "        \n",
    "        x = self.dropout(self.norm(attention + x))\n",
    "        \n",
    "        return self.transformer_block(key, query, x)\n",
    "\n",
    "  \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, embed_dim, seq_len, num_layers=2, expansion_factor=4, heads=8, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # create new embedding\n",
    "        self.word_embedding = nn.Embedding(target_vocab_size, embed_dim)\n",
    "        self.position_embedding = PositionalEncoding(seq_len, embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "                DecoderBlock(embed_dim, expansion_factor=expansion_factor, heads=heads) \n",
    "                for i in range(num_layers)\n",
    "            ])\n",
    "        \n",
    "        # fully connected layer for the end\n",
    "        self.fc = nn.Linear(embed_dim, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x, enc_out, mask=None):\n",
    "        # run embeddings and dropout before looping through decoder blocks\n",
    "        x = self.position_embedding(self.word_embedding(x))\n",
    "        x = self.dropout(x)\n",
    "     \n",
    "        # loop through layers in decoder block\n",
    "        for layer in self.layers:\n",
    "            x = layer(enc_out, x, enc_out, mask) \n",
    "\n",
    "        return torch.nn.functional.softmax(self.fc(x), dim=0)\n",
    "\n",
    "    \n",
    "\n",
    "# connect it all together finally    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_dim, src_vocab_size, target_vocab_size, seq_length,num_layers=2, expansion_factor=4, heads=8):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # connect encoder and decoder\n",
    "        self.encoder = Encoder(seq_length, src_vocab_size, embed_dim, num_layers=num_layers, expansion_factor=expansion_factor, heads=heads)\n",
    "        self.decoder = Decoder(target_vocab_size, embed_dim, seq_length, num_layers=num_layers, expansion_factor=expansion_factor, heads=heads)\n",
    "        \n",
    "    # create matrix with zero's and 1's in a triangle\n",
    "    # for preventing the use of specific words in the transformer when too early\n",
    "    def generate_mask(self, target):\n",
    "        N, l = target.shape\n",
    "       \n",
    "        return torch.tril(torch.ones((l, l))).expand(\n",
    "            N, 1, l, l\n",
    "        )   \n",
    "    \n",
    "    # run everything using mask\n",
    "    def run_transformer(self,source,target):\n",
    "        # encode\n",
    "        encoded = self.encoder(source)\n",
    "        \n",
    "        # create mask\n",
    "        mask = self.generate_mask(target)\n",
    "        \n",
    "        l = []\n",
    "        \n",
    "        out = target\n",
    "        # for each word in the sentence\n",
    "        for i in range(src.shape[1]):\n",
    "            # decode\n",
    "            out = self.decoder(out,encoded,mask) \n",
    "            # cut off end for next round\n",
    "            out = out[:,-1,:]\n",
    "     \n",
    "            out = out.argmax(-1)\n",
    "            # add to list\n",
    "            l.append(out.item())\n",
    "            # fix size\n",
    "            out = torch.unsqueeze(out,axis=0)\n",
    "          \n",
    "        return l\n",
    "    \n",
    "    def forward(self, source, target):\n",
    "        #initialize encoder\n",
    "        encoded = self.encoder(source)\n",
    "        mask = self.generate_mask(target)\n",
    "        # generate decoder\n",
    "        outputs = self.decoder(target, encoded, mask)\n",
    "        return outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "362c4c4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-24T17:45:14.028654Z",
     "iopub.status.busy": "2024-08-24T17:45:14.028121Z",
     "iopub.status.idle": "2024-08-24T17:45:15.119298Z",
     "shell.execute_reply": "2024-08-24T17:45:15.117629Z"
    },
    "papermill": {
     "duration": 1.100586,
     "end_time": "2024-08-24T17:45:15.122495",
     "exception": false,
     "start_time": "2024-08-24T17:45:14.021909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (embedding_layer): Embedding(\n",
       "      (embed): Embedding(11, 512)\n",
       "    )\n",
       "    (positional_encoder): PositionalEncoding()\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): SelfAttention(\n",
       "          (values): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (keys): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (queries): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fully_connected_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (word_embedding): Embedding(11, 512)\n",
       "    (position_embedding): PositionalEncoding()\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderBlock(\n",
       "        (attention): SelfAttention(\n",
       "          (values): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (keys): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (queries): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fully_connected_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (transformer_block): TransformerBlock(\n",
       "          (attention): SelfAttention(\n",
       "            (values): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (keys): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (queries): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fully_connected_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=512, out_features=11, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example run through\n",
    "import random \n",
    "\n",
    "# let 0 be sos token and 1 be eos token\n",
    "source = torch.tensor([[0, random.randint(2, 10), random.randint(2, 10), \n",
    "                        random.randint(2, 10), random.randint(2, 10), \n",
    "                        random.randint(2, 10), random.randint(2, 10),\n",
    "                        random.randint(2, 10), random.randint(2, 10), \n",
    "                        random.randint(2, 10), random.randint(2, 10), 1], \n",
    "                    [0, random.randint(2, 10), random.randint(2, 10), \n",
    "                        random.randint(2, 10), random.randint(2, 10), \n",
    "                        random.randint(2, 10), random.randint(2, 10),\n",
    "                        random.randint(2, 10), random.randint(2, 10), \n",
    "                        random.randint(2, 10), random.randint(2, 10), 1]])\n",
    "target = torch.tensor([[0, random.randint(2, 10), random.randint(2, 10), \n",
    "                        random.randint(2, 10), random.randint(2, 10), \n",
    "                        random.randint(2, 10), random.randint(2, 10),\n",
    "                        random.randint(2, 10), random.randint(2, 10), \n",
    "                        random.randint(2, 10), random.randint(2, 10), 1], \n",
    "                       [0, random.randint(2, 10), random.randint(2, 10), \n",
    "                        random.randint(2, 10), random.randint(2, 10), \n",
    "                        random.randint(2, 10), random.randint(2, 10),\n",
    "                        random.randint(2, 10), random.randint(2, 10), \n",
    "                        random.randint(2, 10), random.randint(2, 10), 1]])\n",
    "\n",
    "\n",
    "# create transformer\n",
    "model = Transformer(embed_dim=512, src_vocab_size=11, \n",
    "                    target_vocab_size=11, seq_length=12,\n",
    "                    num_layers=6, expansion_factor=4, heads=8)\n",
    "\n",
    "out = model(source, target)\n",
    "model\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.830114,
   "end_time": "2024-08-24T17:45:16.352404",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-24T17:45:03.522290",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
